{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM examples\n",
    "\n",
    "## Learning goals: \n",
    "\n",
    "- Understand the concept and mechanisms of \"tools\" in the context of LLMs by examples. \n",
    "\n",
    "In this notebook, we will manually create a mechanism to call an external \"tool\" to complement the LLM's output. This is an exercise to understand the prompting strategy. \n",
    "\n",
    "(In practice, most LLMs now have systematic ways to specify new tools and each provides a specific syntax to specify tools. Also note that working with tools, and some of the related prompting strategies, are very recent and there is still active research in this). \n",
    "\n",
    "## Dependencies\n",
    "\n",
    "We will use the packages `cohere` (see notebook 1) and `openfoodfacts`. Make sure these are installed. \n",
    "\n",
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import openfoodfacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cohere.key\") as f:\n",
    "    COHERE_API_KEY = f.read()\n",
    "cohere_client = cohere.ClientV2(COHERE_API_KEY)\n",
    "api = openfoodfacts.API(user_agent=\"MyAwesomeApp/1.0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tools\n",
    "\n",
    "Imagine we ask Cohere about the calories of some food item. While it will still provide an answer, this answer will be based on the word statistics learned by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the official Oreo website, one regular-sized Oreo cookie (4.6g) contains 28.9 calories. However, the exact number of calories can vary slightly based on the specific variety of Oreo. \n",
      "\n",
      "For example, the Golden Oreo cookies have slightly fewer calories, with one cookie containing around 26 calories, and the Mini Oreos have even fewer, with approximately 19 calories per cookie. \n",
      "\n",
      "The calorie information is usually provided on the packaging of the cookies, so you can verify this information for any specific Oreo product you purchase.\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter some text\")\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }],\n",
    "    model=\"command-r\"\n",
    ")\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, in order to have a more accurate answer, we want to connect this LLM with the OpenFoodFacts database. \n",
    "\n",
    "First, we create a function that queries the database given the name of the product and returns the calories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calories(name):\n",
    "    \"\"\"\n",
    "    Get the calories of a product by its name.\n",
    "    \"\"\"\n",
    "    result = api.product.text_search(name)\n",
    "\n",
    "    # This just takes the first item of the search. \n",
    "    # In a real application, you would need to do more error checking.\n",
    "    first_match = result['products'][0]\n",
    "    id = first_match['_id']\n",
    "    result = api.product.get(id, fields=[\"code\", \"product_name\", \"energy-kcal_100g\"])\n",
    "    return result['energy-kcal_100g']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the following strategy for the LLM to call this function if needed. \n",
    "\n",
    "- We do a first call to the LLM to check if the prompt asks about calories. If not, respond normally. \n",
    "- If the prompt asks about calories, respond with some internal \"code\" so that we can intercept it and do further processing. \n",
    "- If we detect the code, then we call the above function. \n",
    "- Finally, we create a new prompt  asking the LLM to provide a final response based on the original request and the information returned by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<INTERMEDIATE RESULT> Calories= 482\n",
      "Final response:  A bag of Donettes contains 482 calories. This information is according to the OpenFoodFacts database.\n"
     ]
    }
   ],
   "source": [
    "message = input(\"Enter some text\")\n",
    "system_prompt = \"\"\"\n",
    "This is a normal chatbot. Except if it the user asks for the calories of a food item,\n",
    "return the message CALORIES_REQUEST followed by the product name,\n",
    "For example, if the user asks for the calories of an apple, return CALORIES_REQUEST apple.\n",
    "\"\"\"\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = [\n",
    "        {'role':'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': message}\n",
    "    ],\n",
    "    model=\"command-r-plus\"\n",
    ")\n",
    "\n",
    "output = response.message.content[0].text\n",
    "\n",
    "if output.startswith(\"CALORIES_REQUEST\"):\n",
    "    product_name = output.split(\" \")[1]\n",
    "    calories = get_calories(product_name)\n",
    "    print(\"<INTERMEDIATE RESULT> Calories=\", calories)\n",
    "    final_response = cohere_client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "            'role': 'system',\n",
    "            'content': f\"\"\"\n",
    "            Now complete the request of the user, knowing \n",
    "            the calories of the product on the OpenFoodFacts database is {calories}.\n",
    "            Mention this source in the response.\n",
    "            \"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        model=\"command-r-plus\")\n",
    "    print(\"Final response: \", final_response.message.content[0].text)\n",
    "else:\n",
    "    print(\"Final response: \", output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
