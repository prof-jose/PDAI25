{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM examples\n",
    "\n",
    "Learning goals: \n",
    "\n",
    "- Practice different introductory use cases of LLMs\n",
    "\n",
    "## Setup\n",
    "\n",
    "- To run this notebook you will need to create an account in cohere.ai, and get your API Key.\n",
    "- You also need to install the python package `cohere`.\n",
    "\n",
    "## Why cohere? \n",
    "\n",
    "- It was one of the first providers of LLMs as a service, was already in the market before ChatGPT. \n",
    "\n",
    "- Easy to use Python SDK. \n",
    "\n",
    "- A free account gives you access to a Trial API key, which allows 100 calls per minute. \n",
    "\n",
    "- Similar in structure to other APIs and frameworks (if you learn this, you'll navigate the others easily). \n",
    "\n",
    "## Preparing the work\n",
    "\n",
    "Let's initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseantonio.rodriguez15/code/PDAI25/env_proto/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to cohere, navigate to DASHBOARD (at the top), then to API Keys (at the sidebar)\n",
    "\n",
    "# Never store your key in the code or push it to your repository!!!\n",
    "\n",
    "with open(\"cohere.key\") as f:\n",
    "    COHERE_API_KEY = f.read()\n",
    "cohere_client = cohere.ClientV2(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1: Prompt templates\n",
    "\n",
    "### Prompt templates from scratch\n",
    "\n",
    "We can use LLMs to take actions for us and avoid the alternative of writing code. \n",
    "\n",
    "The example below will ask for a text input by the user. Once introduced, it will classify the language of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of the text is English.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Return the language of the text between triple quotation marks: ```!text!``` \" \n",
    "\n",
    "user_text = input(\"Enter some text\")\n",
    "final_prompt = prompt_template.replace(\"!text!\", user_text)\n",
    "\n",
    "formatted_message = {\n",
    "    'role': 'user',\n",
    "    'content': final_prompt \n",
    "}\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    model='command-r-plus',\n",
    "    messages=[formatted_message],\n",
    ")\n",
    "\n",
    "language = response.message.content[0].text\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '270c5101-5551-497a-9089-75f064129574',\n",
       " 'finish_reason': 'COMPLETE',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'The language of the text is English.'}]},\n",
       " 'usage': {'billed_units': {'input_tokens': 21.0, 'output_tokens': 8.0},\n",
       "  'tokens': {'input_tokens': 215.0, 'output_tokens': 8.0}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need a more specific prompt, like one that includes \"reply only with a single word containing the name of the recognized language\" (try that as an exercise). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical workflow would be to use that output as part of the business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language == \"Spanish\":\n",
    "    # Take actions... \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a system prompt\n",
    "\n",
    "All LLMs have converged to a common design where they use roles. Roles can be \"system\" (describing how the assistant should behave), and \"user\" (describing user questions). A reserved role is \"assistant\" (indicating a response by the assistant).\n",
    "\n",
    "This facilitates the usage of LLMs as instruction models and provides a common mechanism for prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FRENCH'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = {\n",
    "    'role': 'system',\n",
    "    'content': \"\"\"\n",
    "        You are a system that classifies the language of the text inputted by the user.\n",
    "        You should read the user input, infer the language in which it is written, and output it.\n",
    "        Respond with a single word indicating the language in upper case.\n",
    "        Respond: UNKNOWN if it is not clear.\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "user_text = input(\"Enter some text\")\n",
    "\n",
    "user_text = {\n",
    "    'role': 'user',\n",
    "    'content': user_text\n",
    "}\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = [system_prompt, user_text],\n",
    "    model='command-r-plus'\n",
    ")\n",
    "\n",
    "response.message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-prompt examples\n",
    "\n",
    "We can use LLMs to \"simulate\" the way AI systems work and provide \"examples\" of what we want to achieve, as part of the prompt. \n",
    "\n",
    "This is know as \"few-shot\", \"in-prompt\" learning. \n",
    "\n",
    "The example below tries to classify the restaurant type based on its name. It does so by preparing a \"template\" of known inputs and outputs, then asking to complete a last input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \"La Taqueria\" Type: \"Mexican\"'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2=\"\"\"\n",
    "Classify the restaurant type based on the restaurant name. \n",
    "Complete the sequence as in the following examples:\n",
    "Name: \"Leche de Tigre\" Type: \"Peruvian\"\n",
    "Name: \"Le Parisien\" Type: \"French\"\n",
    "Name: \"Tapas & Fiesta\" Type: \"Spanish\"\n",
    "Name: \"Rodizio Grill\" Type: \"Brazilian\"\n",
    "Name: \"Shenzen Garden\" Type: \"Chinese\"\n",
    "Name: \"Bella Napoli\" Type: \"Italian\"\n",
    "Name: \"La taqueria Type:\n",
    "\"\"\"\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt2\n",
    "        }\n",
    "    ],\n",
    "    model='command-r-plus'\n",
    ")\n",
    "\n",
    "response.message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind the cost increases with the number of examples. You can get info of the costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"bbeebabd-4748-4073-a85c-16b79d5abb58\",\n",
      "  \"finish_reason\": \"COMPLETE\",\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"Name: \\\"La Taqueria\\\" Type: \\\"Mexican\\\"\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"usage\": {\n",
      "    \"billed_units\": {\n",
      "      \"input_tokens\": 114.0,\n",
      "      \"output_tokens\": 13.0\n",
      "    },\n",
      "    \"tokens\": {\n",
      "      \"input_tokens\": 307.0,\n",
      "      \"output_tokens\": 13.0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 2: LLMs as user interface\n",
    "\n",
    "Natural language can be the new interface. \n",
    "\n",
    "The example below asks the user to write a text specifying whether to turn the lights on or off.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6370f36a3544376b148b099f79dd8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Lights on', indent=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Lights on',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Just for debugging, the output text is false )\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your message\")\n",
    "response = cohere_client.chat(\n",
    "    messages = [{\n",
    "        'role': 'system',\n",
    "        'content': \"\"\"\n",
    "The user will write a text about turning off or on the lights.\n",
    "The user might use different words to describe the action or expressions, \n",
    "like turn on, turn off, activate, deactivate, switch on, switch off, etc.\n",
    "The user might also use different words to describe the lights,\n",
    "like lights, lamps, bulbs, etc.\n",
    "Return a single word: \"true\" if the user wants to turn on the lights, and \"false\" if the user wants to turn them off.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': user_input\n",
    "    }\n",
    "    ],\n",
    "    model = 'command-r-plus'\n",
    ")\n",
    "\n",
    "outcome = response.message.content[0].text\n",
    "print(\"(Just for debugging, the output text is\", outcome, \")\")\n",
    "\n",
    "if outcome == \"true\":\n",
    "    checkbox.value = True\n",
    "elif outcome == \"false\":\n",
    "    checkbox.value = False\n",
    "else:\n",
    "    print(\"Unexpected outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 3: Structuring data\n",
    "\n",
    "Use LLMs to extract information and add it to a certain template. \n",
    "\n",
    "The next example simulates a text from a health record and how the system would extract the necessary information into a template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"age\": 24,\n",
      "  \"temperature\": \"Not found\",\n",
      "  \"reason for visit\": \"Stomach ache\",\n",
      "  \"tests performed\": [\"Not found\"],\n",
      "  \"diagnosis\": \"Not found\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "The user will write a text simulating an electronic health record.\n",
    "Extract the information necessary to populate this json object:\n",
    "{\n",
    "  \"age\": X,\n",
    "  \"temperature\": X,\n",
    "  \"reason for visit\": \"X\",\n",
    "  \"tests performed\": [\"X\", \"X\", \"X\"],\n",
    "    \"diagnosis\": \"X\"\n",
    "}\n",
    "If the information is not present, return \"Not found\".\n",
    "Respond only with the json object. \n",
    "\"\"\"\n",
    "\n",
    "user_input = input(\"Enter your message\")\n",
    "\n",
    "response = cohere_client.chat(\n",
    "    messages = [\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content': system_prompt  \n",
    "      }, \n",
    "      {\n",
    "          'role': 'user',\n",
    "          'content': user_input\n",
    "      }\n",
    "    ],\n",
    "    model='command-r-plus'\n",
    ")\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: Chatbots\n",
    "\n",
    "This example illustrates the use of a chatbot, where the intent to be fulfilled is accomplished after several messages, and each message needs to include the previous chat history. \n",
    "\n",
    "Let's do it in the wrong way first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't know who you are referring to. Could you please provide more context or details about the person you are asking about?\n"
     ]
    }
   ],
   "source": [
    "message1 = {'role': 'user', 'content': 'Who discovered polonium and radium?'}\n",
    "message2 = {'role': 'user', 'content': 'When was she born?'}\n",
    "\n",
    "response1 = cohere_client.chat(messages=[message1], model='command-r-plus')\n",
    "response2 = cohere_client.chat(messages=[message2], model='command-r-plus')\n",
    "\n",
    "print(response2.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text=\"Marie Curie, a Polish and naturalized-French physicist and chemist, discovered polonium and radium. She discovered these elements in 1898 while studying the cause of pitchblende's radioactivity.\")], citations=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the previous chat history, that was stored already in the response json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was born on November 7, 1867, in Warsaw, Poland.\n"
     ]
    }
   ],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages=[\n",
    "        message1,\n",
    "        response1.message,\n",
    "        message2\n",
    "    ],\n",
    "    model='command-r-plus'\n",
    ")\n",
    "print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(id='0aa9da3b-4c1b-4337-aca1-76a5e14560e1', finish_reason='COMPLETE', prompt=None, message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Marie Curie was born on November 7, 1867, in Warsaw, Poland.')], citations=None), usage=Usage(billed_units=UsageBilledUnits(input_tokens=55.0, output_tokens=20.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=254.0, output_tokens=20.0)), logprobs=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
