{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alterative use of the `transformers` library\n",
    "\n",
    "- Learning goal: Illustrate a mode advanced use of the `transformers` library beyond the simple `pipeline` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Image classification model\n",
    "\n",
    "When clicking \"Use in transformers\" in the Huggingface model card e.g. for the ResNet-18 models, you also see a suggestion to use a code similar to the one below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseantonio.rodriguez15/code/PDAI25/env_proto/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses more specific classes (`AutoImageProcessor` and `AutoModelForImageClassification`), that bundle image processing operations, and an image classification model, respectively. \n",
    "\n",
    "This is how you would use these classes e.g. to get the top prediction: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: suspension bridge\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "PATH = \"/Users/joseantonio.rodriguez15/Downloads/images\"\n",
    "full_path = f\"{PATH}/bridge-667997_1280.jpg\"\n",
    "image = Image.open(full_path)\n",
    "\n",
    "model_inputs = processor(image, return_tensors=\"pt\")\n",
    "model_output = model(**model_inputs)\n",
    "\n",
    "logits =  model_output.logits\n",
    "predicted_class_idx = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "label_map = model.config.id2label\n",
    "predicted_class_label = label_map[predicted_class_idx]\n",
    "\n",
    "print(f\"Predicted class label: {predicted_class_label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `model_output` object, we have access to any information of the underlying neural network, such as the logits of each category. \n",
    "\n",
    "The transformers library offers many other classes that facilitate different use cases of working with deep learning models. \n",
    "\n",
    "If you are interested in manipulating models beyond just extracting the predictions, a recommendation is to learn a bit of `pytorch` and the `transformers` library. For illustration, this is how to extract features (as \"embeddings\") using a ResNet model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Map Shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model\n",
    "model_name = \"microsoft/resnet-18\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess image\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Extract feature map (without torch.no_grad())\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Feature map (last hidden state)\n",
    "feature_map = outputs.pooler_output\n",
    "\n",
    "features = feature_map.detach().numpy().flatten()\n",
    "print(\"Feature Map Shape:\", features.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_proto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
